from __future__ import annotations

import pendulum
import pandas as pd
import pymssql
import logging

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.exceptions import AirflowException

# === Config / utils internos ===
from include.utils.pipe_config_loader import PipeConfigLoader
from include.sensors.dataflow_job_name_sensor import DataflowWaitForJobNameSensor  # no usado en este DAG
from include.utils.qa_validations import validate_pipeline  # no usado en este DAG
from include.utils.sqlserver_connection import get_sql_config

# === Cargar configuración de entorno para evitar hardcoding ===
TABLE_TYPE = "sqlserver_objects"  # ajusta si en tu repo usas otro valor

config_loader = PipeConfigLoader()
env_config = config_loader.load_configurations(TABLE_TYPE)

project_id = env_config["project_id"]
dataops_dataset = env_config["dataops_dataset"]
object_extraction_metadata_table = env_config["object_extraction_metadata_table_name"]
region = env_config["region"]
composer_gcs_bucket = env_config["composer_gcs_bucket"]

# lookuptable_fk_pk_analysis = env_config["lookuptable_fk_pk_analysis"]
# lookuptable_fk_pk_analysis_staging = env_config["lookuptable_fk_pk_analysis_staging"]
# lookuptable_fk_pk_enriched = env_config["lookuptable_fk_pk_enriched"]
# lookuptable_fk_pk = env_config["lookuptable_fk_pk"]
# lookup_table_relationship = env_config["lookup_table_relationship"]


secret_id = "rxo-dataeng-datalake-np-brokerage-fo-mssql-xpomaster-uat-creds-connection-string"
bq_final_table = f"{project_id}.{dataops_dataset}.lookuptable_fk_pk_analysis"
bq_stage_table = f"{project_id}.{dataops_dataset}.lookuptable_fk_pk_analysis_staging"
bq_enriched_table = f"{project_id}.{dataops_dataset}.lookuptable_fk_pk_enriched"
bq_lookup_table = f"{project_id}.{dataops_dataset}.lookuptable_fk_pk"
bq_rel_table = f"{project_id}.{dataops_dataset}.lookup_table_relationship"
bq_metadata_table = f"{project_id}.{dataops_dataset}.{object_extraction_metadata_table}"

# === TASK 0: Drop lookup table at the beginning (optional reset) ===
def drop_loouk_table_at_start(**context):
    """
    Drop the final lookup table before starting the pipeline.
    This is optional and mainly useful when you want a clean rebuild.
    """
    bq_hook = BigQueryHook(
        project_id=project_id,
        location=region
    )
    client = bq_hook.get_client()
    try:
        client.delete_table(bq_lookup_table, not_found_ok=True)
        logging.info(f"Dropped lookup table {bq_lookup_table} (if existed).")
    except Exception as e:
        logging.error(f"Error deleting lookup table {bq_lookup_table}: {e}")


# === TASK 1: Extract metadata from BigQuery and push to XCom ===
def extract_and_process_metadata(**context):
    """
    Connects to BigQuery, extracts metadata (database_name, schema_name, table_name)
    from extraction_metadata, and stores it in a Pandas DataFrame.
    Pushes the rows as list[dict] to XCom.
    """
    bq_hook = BigQueryHook(
        project_id=project_id,
        location=region,
    )

    # Usamos la tabla desde la configuración (normalmente ya es project.dataset.table)
    source_type_filter = "sqlserver"
    table_type_filter = "extraction"
    print("##############")
    print(bq_metadata_table)
    sql_query = f"""
    SELECT
        JSON_EXTRACT_SCALAR(source_config, '$.database_name') AS database_name,
        JSON_EXTRACT_SCALAR(source_config, '$.schema_name')   AS schema_name,
        JSON_EXTRACT_SCALAR(source_config, '$.table_name')    AS table_name
    FROM `{bq_metadata_table}`
    WHERE
        source_type = '{source_type_filter}'
        AND table_type = '{table_type_filter}'
        AND enabled is TRUE
    """

    logging.info(f"Executing BigQuery SQL Query:\n{sql_query}")

    try:
        client = bq_hook.get_client()
        job = client.query(sql_query, location=region)
        rows = list(job.result())
    except AirflowException as e:
        logging.error(f"Airflow Exception while executing BigQuery query: {e}")
        raise
    except Exception as e:
        logging.error(f"Unexpected error while executing BigQuery query: {e}")
        raise

    rows_dicts = [dict(r) for r in rows]
    df = pd.DataFrame(rows_dicts)

    if df.empty:
        logging.warning("No records found matching the criteria in extraction_metadata.")
    else:
        logging.info("Metadata extracted and processed into DataFrame:")
        logging.info(df.head())
        logging.info(f"Total rows: {len(df)}")

    context["ti"].xcom_push(key="metadata_rows", value=rows_dicts)


# === TASK 2: Run SQL Server queries in batches (PK + rowcount + DISTINCT) ===
def build_and_run_sqlserver_query(**context):
    """
    1) Pulls metadata from XCom.
    2) Deduplicates (database_name, schema_name, table_name) and processes them in batches of 10.
    3) For each batch and each database_name:
       - Queries PKs (columns ending in ...Id + is_pk flag).
       - Queries total rowcount per table.
    4) Merges PK + total_rows.
    5) For each (Database, Schema, Table, column_id) executes:
         SELECT COUNT(DISTINCT [column_id]) ...
       and adds the distinct_values column.
    6) Pushes df_final to XCom.
    """

    ti = context["ti"]
    metadata_rows = ti.xcom_pull(
        task_ids="extract_and_process_sqlserver_metadata",
        key="metadata_rows",
    )

    if not metadata_rows:
        logging.warning("No metadata_rows received from XCom. Nothing to do.")
        return

    df_meta = pd.DataFrame(metadata_rows)
    logging.info("Full metadata from BigQuery (sample):")
    logging.info(df_meta.head())

    # Drop rows with null database/schema/table
    df_meta = df_meta.dropna(subset=["database_name", "schema_name", "table_name"]).copy()

    if df_meta.empty:
        logging.warning("No valid rows with non-null database/schema/table. Nothing to do.")
        return

    # Normalize strings and deduplicate triplets
    df_unique = (
        df_meta.assign(
            database_name=lambda d: d["database_name"].astype(str).str.strip(),
            schema_name=lambda d: d["schema_name"].astype(str).str.strip(),
            table_name=lambda d: d["table_name"].astype(str).str.strip(),
        )[["database_name", "schema_name", "table_name"]]
        .drop_duplicates()
        .reset_index(drop=True)
    )

    logging.info(f"Unique triplets (db, schema, table): {len(df_unique)}")

    # Whitelist for DBs
    allowed_dbs = {"XpoMaster", "brkLTL", "XPOCustomer"}
    df_unique = df_unique[df_unique["database_name"].isin(allowed_dbs)].reset_index(drop=True)
    logging.info(f"Unique triplets after allowed_dbs filter: {len(df_unique)}")

    if df_unique.empty:
        logging.warning("No rows after filtering by allowed_dbs.")
        return

    batch_size = 10

    pk_dfs = []
    cnt_dfs = []

    conn = None
    try:
        # === NUEVO: obtener configuración SQL Server desde Secret Manager ===
        cfg = get_sql_config(secret_id, project_id)

        # Resolver servidor con la misma prioridad que build_connection_string
        server = cfg.get("privateIPaddress") or cfg.get("publicIPaddress") or cfg.get("server")
        database = cfg.get("database", "XpoMaster")
        username = cfg["username"]
        password = cfg["password"]

        conn = pymssql.connect(
            server=server,
            user=username,
            password=password,
            database=database,
        )
        cursor = conn.cursor()

        # Iterate in batches of 10 tables
        for start in range(0, len(df_unique), batch_size):
            df_batch = df_unique.iloc[start:start + batch_size].copy()
            logging.info(f"\n=== Processing batch rows {start} to {start + len(df_batch) - 1} ===")
            logging.info(df_batch)

            dbs_in_batch = sorted(df_batch["database_name"].unique())
            logging.info(f"Databases in this batch: {dbs_in_batch}")

            for db in dbs_in_batch:
                df_db = df_batch[df_batch["database_name"] == db].copy()
                if df_db.empty:
                    continue

                logging.info(f"\n--- Processing DB: {db} in this batch ---")

                values_rows = []
                for _, row in df_db.iterrows():
                    sch = row["schema_name"]
                    tbl = row["table_name"]

                    db_esc = db.replace("'", "''")
                    sch_esc = sch.replace("'", "''")
                    tbl_esc = tbl.replace("'", "''")

                    values_rows.append(
                        f"(N'{db_esc}', N'{sch_esc}', N'{tbl_esc}')"
                    )

                values_clause = ",\n        ".join(values_rows)

                # === PK query (CTE-only approach) ===
                pk_tsql = f"""
WITH todo AS (
    SELECT DISTINCT
        LTRIM(RTRIM(DatabaseName)) AS DatabaseName,
        LTRIM(RTRIM(SchemaName))   AS SchemaName,
        LTRIM(RTRIM(TableName))    AS TableName
    FROM (VALUES
        {values_clause}
    ) v(DatabaseName, SchemaName, TableName)
),
cols AS (
    SELECT 
        DB      = N'{db_esc}',
        s.name  AS [Schema],
        t.name  AS [Table],
        c.name  AS column_id
    FROM [{db_esc}].sys.tables t
    JOIN [{db_esc}].sys.schemas s ON s.schema_id = t.schema_id
    JOIN [{db_esc}].sys.columns c ON c.object_id = t.object_id
    JOIN todo td
      ON td.SchemaName = s.name
     AND td.TableName  = t.name
    WHERE c.name LIKE N'%Id'
),
pk_cols AS (
    SELECT 
        s.name AS [Schema],
        t.name AS [Table],
        c.name AS column_id
    FROM [{db_esc}].sys.tables t
    JOIN [{db_esc}].sys.schemas s ON s.schema_id = t.schema_id
    JOIN [{db_esc}].sys.key_constraints kc
      ON kc.parent_object_id = t.object_id AND kc.type = 'PK'
    JOIN [{db_esc}].sys.index_columns ic
      ON ic.object_id = t.object_id AND ic.index_id = kc.unique_index_id
    JOIN [{db_esc}].sys.columns c
      ON c.object_id = ic.object_id AND c.column_id = ic.column_id
),
pick AS (
    SELECT 
        cols.DB      AS [Database],
        cols.[Schema],
        cols.[Table],
        cols.column_id,
        is_pk = CASE WHEN pk_cols.column_id IS NOT NULL THEN 1 ELSE 0 END
    FROM cols
    LEFT JOIN pk_cols
      ON pk_cols.[Schema]   = cols.[Schema]
     AND pk_cols.[Table]    = cols.[Table]
     AND pk_cols.column_id  = cols.column_id
)
SELECT [Database], [Schema], [Table], column_id, is_pk
FROM pick
ORDER BY [Database], [Schema], [Table], column_id;
"""

                logging.info("PK T-SQL for this DB:")
                logging.info(pk_tsql)

                cursor.execute(pk_tsql)
                rows_pk = cursor.fetchall()
                if rows_pk:
                    cols_pk = [col[0] for col in cursor.description]
                    df_pk_batch_db = pd.DataFrame.from_records(rows_pk, columns=cols_pk)
                    pk_dfs.append(df_pk_batch_db)
                    logging.info(f"PK rows for DB {db}: {len(df_pk_batch_db)}")
                else:
                    logging.warning(f"PK query returned no rows for DB {db}.")

                # === Rowcount query per table (CTE-only approach) ===
                counts_tsql = f"""
WITH todo AS (
    SELECT DISTINCT
        LTRIM(RTRIM(DatabaseName)) AS DatabaseName,
        LTRIM(RTRIM(SchemaName))   AS SchemaName,
        LTRIM(RTRIM(TableName))    AS TableName
    FROM (VALUES
        {values_clause}
    ) v(DatabaseName, SchemaName, TableName)
)
SELECT
    [Database]        = N'{db_esc}',
    [Schema]          = s.name,
    [Table]           = t.name,
    total_rows        = SUM(p.[rows])
FROM [{db_esc}].sys.tables t
JOIN [{db_esc}].sys.schemas s    ON s.schema_id = t.schema_id
JOIN [{db_esc}].sys.partitions p ON p.object_id = t.object_id
JOIN todo td
  ON td.SchemaName = s.name
 AND td.TableName  = t.name
WHERE p.index_id IN (0, 1)
GROUP BY s.name, t.name
ORDER BY [Database], [Schema], [Table];
"""

                logging.info("COUNTS T-SQL for this DB:")
                logging.info(counts_tsql)

                cursor.execute(counts_tsql)
                rows_cnt = cursor.fetchall()
                if rows_cnt:
                    cols_cnt = [col[0] for col in cursor.description]
                    df_cnt_batch_db = pd.DataFrame.from_records(rows_cnt, columns=cols_cnt)
                    cnt_dfs.append(df_cnt_batch_db)
                    logging.info(f"Rowcount rows for DB {db}: {len(df_cnt_batch_db)}")
                else:
                    logging.warning(f"Rowcount query returned no rows for DB {db}.")

        # Concatenate PK and COUNTS results
        if pk_dfs:
            df_pk_all = pd.concat(pk_dfs, ignore_index=True)
        else:
            df_pk_all = pd.DataFrame(columns=["Database", "Schema", "Table", "column_id", "is_pk"])

        if cnt_dfs:
            df_cnt_all = pd.concat(cnt_dfs, ignore_index=True)
        else:
            df_cnt_all = pd.DataFrame(columns=["Database", "Schema", "Table", "total_rows"])

        logging.info("\nGlobal PK DF (sample):")
        logging.info(df_pk_all.head())
        logging.info(f"Global PK rows: {len(df_pk_all)}")

        logging.info("\nGlobal COUNTS DF (sample):")
        logging.info(df_cnt_all.head())
        logging.info(f"Global rowcount rows: {len(df_cnt_all)}")

        # Join by table: PK + total_rows
        if not df_pk_all.empty and not df_cnt_all.empty:
            df_final = df_pk_all.merge(
                df_cnt_all,
                on=["Database", "Schema", "Table"],
                how="left",
            )
        else:
            df_final = df_pk_all.copy()
            if "total_rows" not in df_final.columns:
                df_final["total_rows"] = pd.NA

        # === COUNT(DISTINCT) per column_id ===
        distinct_rows = []

        for _, r in (
            df_final[["Database", "Schema", "Table", "column_id"]]
            .dropna()
            .drop_duplicates()
            .iterrows()
        ):
            db = r["Database"]
            sch = r["Schema"]
            tbl = r["Table"]
            col = r["column_id"]

            db_esc = str(db).replace("'", "''")
            sch_esc = str(sch).replace("'", "''")
            tbl_esc = str(tbl).replace("'", "''")
            col_esc = str(col).replace("'", "''")

            distinct_sql = f"""
SELECT
    DISTINCT_COUNT = COUNT(DISTINCT [{col_esc}])
FROM [{db_esc}].[{sch_esc}].[{tbl_esc}];
"""
            logging.info("DISTINCT T-SQL:")
            logging.info(distinct_sql)

            try:
                cursor.execute(distinct_sql)
                row = cursor.fetchone()
                distinct_count = row[0] if row else None
            except Exception as e:
                logging.error(
                    f"Error computing DISTINCT for "
                    f"{db}.{sch}.{tbl}.{col}: {e}"
                )
                distinct_count = None

            distinct_rows.append(
                {
                    "Database": db,
                    "Schema": sch,
                    "Table": tbl,
                    "column_id": col,
                    "distinct_values": distinct_count,
                }
            )

        if distinct_rows:
            df_distinct = pd.DataFrame(distinct_rows)
        else:
            df_distinct = pd.DataFrame(
                columns=["Database", "Schema", "Table", "column_id", "distinct_values"]
            )

        logging.info("\nDistinct counts DF (sample):")
        logging.info(df_distinct.head())
        logging.info(f"Distinct rows: {len(df_distinct)}")

        # Final merge: PK + total_rows + distinct_values
        if not df_distinct.empty:
            df_final = df_final.merge(
                df_distinct,
                on=["Database", "Schema", "Table", "column_id"],
                how="left",
            )
        else:
            df_final["distinct_values"] = pd.NA

        logging.info("\nFinal merged result (PK + total_rows + distinct_values) (sample):")
        logging.info(df_final.head())
        logging.info(f"Total merged rows: {len(df_final)}")

        # Push final result to XCom
        ti.xcom_push(
            key="pk_with_rowcounts_and_distinct",
            value=df_final.to_dict(orient="records"),
        )

    finally:
        if conn is not None:
            conn.close()


# === TASK 3: Write final result into BigQuery with upsert (lookuptable_fk_pk_analysis) ===
def write_results_to_bigquery(**context):
    """
    Reads the final result from XCom, loads it into a staging table in BigQuery,
    ensures the final table exists, and performs a MERGE (upsert) based on
    (database_name, schema_name, table_name, column_name).
    """

    ti = context["ti"]
    records = ti.xcom_pull(
        task_ids="build_and_run_sqlserver_pk_query_dynamic_batches",
        key="pk_with_rowcounts_and_distinct",
    )

    if not records:
        logging.warning("No final records found in XCom. Nothing to write to BigQuery.")
        return

    df = pd.DataFrame(records)
    if df.empty:
        logging.warning("Final DataFrame is empty. Nothing to write to BigQuery.")
        return

    # Rename columns to match BigQuery schema
    df_bq = df.rename(
        columns={
            "Database": "database_name",
            "Schema": "schema_name",
            "Table": "table_name",
            "column_id": "column_name",
        }
    )[
        [
            "database_name",
            "schema_name",
            "table_name",
            "column_name",
            "is_pk",
            "total_rows",
            "distinct_values",
        ]
    ]

    # Enforce numeric types where appropriate
    df_bq["is_pk"] = pd.to_numeric(df_bq["is_pk"], errors="coerce").astype("Int64")
    df_bq["total_rows"] = pd.to_numeric(df_bq["total_rows"], errors="coerce").astype("Int64")
    df_bq["distinct_values"] = pd.to_numeric(df_bq["distinct_values"], errors="coerce").astype("Int64")

    # Load timestamp
    df_bq["load_ts"] = pd.Timestamp.utcnow()

    logging.info("DataFrame to load into BigQuery (sample):")
    logging.info(df_bq.head())
    logging.info(f"Rows to write: {len(df_bq)}")

    bq_hook = BigQueryHook(
        project_id=project_id,
        location=region,
    )
    client = bq_hook.get_client()

    # 1) Load into staging table (created if it does not exist)
    load_job = client.load_table_from_dataframe(
        df_bq,
        bq_stage_table,
    )
    load_job.result()
    logging.info(f"Loaded {len(df_bq)} rows into staging table {bq_stage_table}.")

    # 2) Ensure final analysis table exists
    ddl = f"""
    CREATE TABLE IF NOT EXISTS `{bq_final_table}` (
      database_name   STRING,
      schema_name     STRING,
      table_name      STRING,
      column_name     STRING,
      is_pk           INT64,
      total_rows      INT64,
      distinct_values INT64,
      load_ts         TIMESTAMP
    )
    """
    client.query(ddl, location=region).result()
    logging.info(f"Ensured final table {bq_final_table} exists.")

    # 3) MERGE (upsert) from staging into final
    merge_sql = f"""
    MERGE `{bq_final_table}` T
    USING `{bq_stage_table}` S
    ON T.database_name = S.database_name
       AND T.schema_name = S.schema_name
       AND T.table_name = S.table_name
       AND T.column_name = S.column_name
    WHEN MATCHED AND (
        T.is_pk           IS DISTINCT FROM S.is_pk OR
        T.total_rows      IS DISTINCT FROM S.total_rows OR
        T.distinct_values IS DISTINCT FROM S.distinct_values
    ) THEN
      UPDATE SET
        T.is_pk           = S.is_pk,
        T.total_rows      = S.total_rows,
        T.distinct_values = S.distinct_values,
        T.load_ts         = S.load_ts
    WHEN NOT MATCHED THEN
      INSERT (database_name, schema_name, table_name, column_name,
              is_pk, total_rows, distinct_values, load_ts)
      VALUES (S.database_name, S.schema_name, S.table_name, S.column_name,
              S.is_pk, S.total_rows, S.distinct_values, S.load_ts);
    """
    client.query(merge_sql, location=region).result()
    logging.info(f"Merge completed into {bq_final_table}.")

    # 4) Drop staging table
    try:
        client.delete_table(bq_stage_table, not_found_ok=True)
        logging.info(f"Staging table {bq_stage_table} dropped.")
    except Exception as e:
        logging.error(f"Error dropping staging table {bq_stage_table}: {e}")


# === TASK 4: Call SPs, create final lookup table, compute scores, clean intermediate tables ===
def run_sp_and_cleanup(**context):
    """
    1) Calls sp_build_lookup_fk_pk_enriched() to build lookuptable_fk_pk_enriched
       from lookuptable_fk_pk_analysis.
    2) Creates/overwrites lookuptable_fk_pk from lookuptable_fk_pk_enriched.
    3) Calls sp_enrich_pk_scores() to compute and write scores into lookuptable_fk_pk.
    4) Drops lookuptable_fk_pk_analysis and lookuptable_fk_pk_enriched.
    """

    bq_hook = BigQueryHook(
        project_id=project_id,
        location=region,
    )
    client = bq_hook.get_client()

    # 1) Call SP that builds the enriched table
    sp_sql_enriched = f"""
    CALL `{project_id}.{dataops_dataset}.sp_build_lookup_fk_pk_enriched`();
    """
    logging.info("Calling stored procedure sp_build_lookup_fk_pk_enriched...")
    client.query(sp_sql_enriched, location=region).result()
    logging.info("Stored procedure sp_build_lookup_fk_pk_enriched completed.")

    # 2) Create/replace final lookup table from enriched
    ddl_create_final = f"""
    CREATE OR REPLACE TABLE `{bq_lookup_table}` AS
    SELECT *
    FROM `{bq_enriched_table}`;
    """
    client.query(ddl_create_final, location=region).result()
    logging.info(f"Created/overwritten final lookup table {bq_lookup_table} from {bq_enriched_table}.")

    # 3) Call SP that computes scores on the final lookup table
    sp_sql_scores = f"""
    CALL `{project_id}.{dataops_dataset}.sp_enrich_pk_scores`();
    """
    logging.info("Calling stored procedure sp_enrich_pk_scores to compute scores...")
    client.query(sp_sql_scores, location=region).result()
    logging.info("Stored procedure sp_enrich_pk_scores completed.")

    # 4) Drop analysis table
    try:
        client.delete_table(bq_final_table, not_found_ok=True)
        logging.info(f"Analysis table {bq_final_table} dropped.")
    except Exception as e:
        logging.error(f"Error dropping analysis table {bq_final_table}: {e}")

    # 5) Drop enriched table
    try:
        client.delete_table(bq_enriched_table, not_found_ok=True)
        logging.info(f"Enriched table {bq_enriched_table} dropped.")
    except Exception as e:
        logging.error(f"Error dropping enriched table {bq_enriched_table}: {e}")


# === TASK 5: Add lookup flags/labels based on scores ===
def compute_lookup_flags(**context):
    """
    1) Ensures the columns is_lookup_flag (BOOL) and lookup_class (STRING)
       exist in lookuptable_fk_pk.
    2) Updates those columns using a cutoff based on pk_lookup_score, pk_size_class,
       is_pk_pk and pk_num_fk_columns to identify lookup tables.
    """

    bq_hook = BigQueryHook(
        project_id=project_id,
        location=region,
    )
    client = bq_hook.get_client()

    # 1) Add columns if they do not exist
    alter_is_lookup = f"""
    ALTER TABLE `{bq_lookup_table}`
    ADD COLUMN IF NOT EXISTS is_lookup_flag BOOL;
    """
    client.query(alter_is_lookup, location=region).result()
    logging.info(f"Ensured column is_lookup_flag exists on {bq_lookup_table}.")

    alter_lookup_class = f"""
    ALTER TABLE `{bq_lookup_table}`
    ADD COLUMN IF NOT EXISTS lookup_class STRING;
    """
    client.query(alter_lookup_class, location=region).result()
    logging.info(f"Ensured column lookup_class exists on {bq_lookup_table}.")

    # 2) Update flags and classes based on cutoff logic
    update_sql = f"""
    UPDATE `{bq_lookup_table}`
    SET
      is_lookup_flag = CASE
        WHEN is_pk_pk = 1
         AND pk_num_fk_columns >= 1
         AND pk_size_class IN ('SMALL', 'MEDIUM')
         AND pk_lookup_score >= 0.6
        THEN TRUE
        ELSE FALSE
      END,
      lookup_class = CASE
        WHEN is_pk_pk = 1
         AND pk_num_fk_columns >= 1
         AND pk_size_class IN ('SMALL', 'MEDIUM')
         AND pk_lookup_score >= 0.75
        THEN 'STRONG'
        WHEN is_pk_pk = 1
         AND pk_num_fk_columns >= 1
         AND pk_size_class IN ('SMALL', 'MEDIUM')
         AND pk_lookup_score >= 0.5
        THEN 'CANDIDATE'
        ELSE 'OTHER'
      END
    WHERE TRUE
    """
    client.query(update_sql, location=region).result()
    logging.info(f"Updated is_lookup_flag and lookup_class on {bq_lookup_table}.")


# === TASK 6: Check lookup candidates vs lookup_table_relationships and log if missing ===
def check_lookup_candidates_and_log(**context):
    """
    1) Selects rows in lookuptable_fk_pk where lookup_class = 'CANDIDATE' and is_pk_pk = 1.
    2) LEFT JOINs them with lookup_table_relationships on (database_pk, schema_pk, table_pk, pk_name_id).
    3) Filters rows where data_to_add_into_pipeline IS NULL.
    4) If any are missing, logs a single ERROR entry with a recognizable prefix,
       so that Cloud Monitoring can trigger an alert based on this log.
    """

    bq_hook = BigQueryHook(
        project_id=project_id,
        location=region,
    )
    client = bq_hook.get_client()

    sql_candidates = f"""
    WITH candidates AS (
      SELECT
        database_name_pk   AS database_name,
        schema_name_pk     AS schema_name,
        table_name_pk      AS table_name,
        column_name_pk     AS column_name
      FROM `{bq_lookup_table}`
      WHERE lookup_class = 'CANDIDATE'
        AND is_pk_pk = 1
    ),
    rel AS (
      SELECT
        database_pk,
        schema_pk,
        table_pk,
        pk_name_id,
        `DATA TO ADD INTO PIPELINE` as data_to_add_into_pipeline
      FROM `{bq_rel_table}`
    )
    SELECT
      c.database_name,
      c.schema_name,
      c.table_name,
      c.column_name,
      r.data_to_add_into_pipeline
    FROM candidates c
    LEFT JOIN rel r
      ON r.database_pk = c.database_name
     AND r.schema_pk   = c.schema_name
     AND r.table_pk    = c.table_name
     AND r.pk_name_id  = c.column_name
    WHERE r.data_to_add_into_pipeline IS NULL
    """

    logging.info("Checking lookup candidates against lookup_table_relationships...")
    job = client.query(sql_candidates, location=region)
    rows = list(job.result())

    if not rows:
        logging.info("LOOKUP_CANDIDATES_CHECK: no missing data_to_add_into_pipeline.")
        return

    missing_entries = [
        f"{r['database_name']}.{r['schema_name']}.{r['table_name']}.{r['column_name']}"
        for r in rows
    ]
    msg = "LOOKUP_CANDIDATES_MISSING_DATA: " + ", ".join(missing_entries)

    # This is the log line that Monitoring will use for alerting.
    logging.error(msg)

# === DAG definition ===
with DAG(
    dag_id="bq_to_sqlserver_pk_catalog_pymssql_batches_cte_only_distinct_to_bq_v10",
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    catchup=False,
    schedule=None,
    tags=[
        "bigquery",
        "metadata",
        "sqlserver",
        "pk_catalog",
        "rowcount",
        "distinct_values",
        "dynamic",
        "batches",
        "bq_upsert",
        "sp_enrich",
        "lookup_alerts",
    ],
) as dag:

    drop_loouk_table_task = PythonOperator(
        task_id="drop_looukp_table_at_start",
        python_callable=drop_loouk_table_at_start,
        provide_context=True,
    )

    extract_metadata_task = PythonOperator(
        task_id="extract_and_process_sqlserver_metadata",
        python_callable=extract_and_process_metadata,
        provide_context=True,
    )

    run_sqlserver_pk_task = PythonOperator(
        task_id="build_and_run_sqlserver_pk_query_dynamic_batches",
        python_callable=build_and_run_sqlserver_query,
        provide_context=True,
    )

    write_bq_task = PythonOperator(
        task_id="write_pk_analysis_to_bigquery",
        python_callable=write_results_to_bigquery,
        provide_context=True,
    )

    sp_and_cleanup_task = PythonOperator(
        task_id="run_sp_and_cleanup_lookup_table",
        python_callable=run_sp_and_cleanup,
        provide_context=True,
    )

    compute_lookup_flags_task = PythonOperator(
        task_id="compute_lookup_flags_on_lookup_table",
        python_callable=compute_lookup_flags,
        provide_context=True,
    )

    check_lookup_candidates_task = PythonOperator(
        task_id="check_lookup_candidates_and_log",
        python_callable=check_lookup_candidates_and_log,
        provide_context=True,
    )

    # End-to-end dependency chain
    (
        drop_loouk_table_task
        >> extract_metadata_task
        >> run_sqlserver_pk_task
        >> write_bq_task
        >> sp_and_cleanup_task
        >> compute_lookup_flags_task
        >> check_lookup_candidates_task
    )
